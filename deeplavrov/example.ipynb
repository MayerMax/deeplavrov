{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "root_folders = ['..', '../..', './']\n",
    "root_folders = [os.path.abspath(os.path.join(x)) for x in root_folders]\n",
    "for path in root_folders:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from deeplavrov.models.wordlevel import AttentionRNNTranslator\n",
    "from deeplavrov.vocabulary.vocabulary import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336666, 336666)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# клетка для того, чтобы создать файлы в нужном формате\n",
    "def make_pairs_from_anki(path, num_examples=None):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        if num_examples:\n",
    "            pairs = [x.split('\\t') for x in data[:min(len(data), num_examples)]]\n",
    "        else:\n",
    "            pairs = [x.split('\\t') for x in data]\n",
    "      \n",
    "    return [x[0].strip() for x in pairs], [x[1].strip() for x in pairs]\n",
    "\n",
    "path = os.path.join('rus-eng', 'rus.txt')\n",
    "num_examples = None\n",
    "input_lang, target_lang = make_pairs_from_anki(path, num_examples)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "input_lang_train, input_lang_val, target_lang_train, target_lang_val = train_test_split(input_lang, target_lang, test_size=0.1, shuffle=True)\n",
    "\n",
    "with open('rus-eng/eng_anki_train.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(input_lang_train).strip())\n",
    "\n",
    "with open('rus-eng/eng_anki_val.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(input_lang_val).strip())\n",
    "\n",
    "with open('rus-eng/ru_anki_train.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(target_lang_train).strip())\n",
    "\n",
    "with open('rus-eng/ru_anki_val.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(target_lang_val).strip())\n",
    "\n",
    "len(input_lang), len(target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = AttentionRNNTranslator(batch_size=16, num_epochs=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab took 23.153021097183228 second(s)\n",
      "Building vocab took 23.65468144416809 second(s)\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1 Batch 0 Loss 1.6181\n",
      "Epoch 1 Batch 100 Loss 0.8936\n",
      "Epoch 1 Batch 200 Loss 1.0823\n",
      "Epoch 1 Batch 300 Loss 0.7596\n",
      "Epoch 1 Batch 400 Loss 0.6659\n",
      "Epoch 1 Batch 500 Loss 0.7487\n",
      "Epoch 1 Batch 600 Loss 0.7623\n",
      "Epoch 1 Batch 700 Loss 0.7619\n",
      "Epoch 1 Batch 800 Loss 0.6047\n",
      "Epoch 1 Batch 900 Loss 0.6031\n",
      "Epoch 1 Batch 1000 Loss 0.6449\n",
      "Epoch 1 Batch 1100 Loss 0.7080\n",
      "Epoch 1 Batch 1200 Loss 0.7577\n",
      "Epoch 1 Batch 1300 Loss 0.7024\n",
      "Epoch 1 Batch 1400 Loss 0.6925\n",
      "Epoch 1 Batch 1500 Loss 0.7463\n",
      "Epoch 1 Batch 1600 Loss 0.7486\n",
      "Epoch 1 Batch 1700 Loss 0.6005\n",
      "Epoch 1 Batch 1800 Loss 0.7463\n",
      "Epoch 1 Batch 1900 Loss 0.8770\n",
      "Epoch 1 Batch 2000 Loss 0.6397\n",
      "Epoch 1 Batch 2100 Loss 0.6888\n",
      "Epoch 1 Batch 2200 Loss 0.6732\n",
      "Epoch 1 Batch 2300 Loss 0.6601\n",
      "Epoch 1 Batch 2400 Loss 0.7935\n",
      "Epoch 1 Batch 2500 Loss 0.5601\n",
      "Epoch 1 Batch 2600 Loss 0.6702\n",
      "Epoch 1 Batch 2700 Loss 0.5737\n",
      "Epoch 1 Batch 2800 Loss 0.7322\n",
      "Epoch 1 Batch 2900 Loss 0.8741\n",
      "Epoch 1 Batch 3000 Loss 0.8709\n",
      "Epoch 1 Batch 3100 Loss 0.5958\n",
      "Epoch 1 Batch 3200 Loss 0.6820\n",
      "Epoch 1 Batch 3300 Loss 0.4902\n",
      "Epoch 1 Batch 3400 Loss 0.6093\n",
      "Epoch 1 Batch 3500 Loss 0.5958\n",
      "Epoch 1 Batch 3600 Loss 0.5992\n",
      "Epoch 1 Batch 3700 Loss 0.5591\n",
      "Epoch 1 Batch 3800 Loss 0.6311\n",
      "Epoch 1 Batch 3900 Loss 0.7021\n",
      "Epoch 1 Batch 4000 Loss 0.5425\n",
      "Epoch 1 Batch 4100 Loss 0.5963\n",
      "Epoch 1 Batch 4200 Loss 0.6115\n",
      "Epoch 1 Batch 4300 Loss 0.6421\n",
      "Epoch 1 Batch 4400 Loss 0.4522\n",
      "Epoch 1 Batch 4500 Loss 0.5117\n",
      "Epoch 1 Batch 4600 Loss 0.7321\n",
      "Epoch 1 Batch 4700 Loss 0.4745\n",
      "Epoch 1 Batch 4800 Loss 0.5272\n",
      "Epoch 1 Batch 4900 Loss 0.5297\n",
      "Epoch 1 Batch 5000 Loss 0.4303\n",
      "Epoch 1 Batch 5100 Loss 0.5623\n",
      "Epoch 1 Batch 5200 Loss 0.5257\n",
      "Epoch 1 Batch 5300 Loss 0.6069\n",
      "Epoch 1 Batch 5400 Loss 0.5411\n",
      "Epoch 1 Batch 5500 Loss 0.5334\n",
      "Epoch 1 Batch 5600 Loss 0.5525\n",
      "Epoch 1 Batch 5700 Loss 0.5627\n",
      "Epoch 1 Batch 5800 Loss 0.6918\n",
      "Epoch 1 Batch 5900 Loss 0.5428\n",
      "Epoch 1 Batch 6000 Loss 0.4611\n",
      "Epoch 1 Batch 6100 Loss 0.5233\n",
      "Epoch 1 Batch 6200 Loss 0.6561\n",
      "Epoch 1 Batch 6300 Loss 0.6734\n",
      "Epoch 1 Batch 6400 Loss 0.5663\n",
      "Epoch 1 Batch 6500 Loss 0.4554\n",
      "Epoch 1 Batch 6600 Loss 0.5061\n",
      "Epoch 1 Batch 6700 Loss 0.7025\n",
      "Epoch 1 Batch 6800 Loss 0.6670\n",
      "Epoch 1 Batch 6900 Loss 0.4707\n",
      "Epoch 1 Batch 7000 Loss 0.4797\n",
      "Epoch 1 Batch 7100 Loss 0.4903\n",
      "Epoch 1 Batch 7200 Loss 0.4937\n",
      "Epoch 1 Batch 7300 Loss 0.4880\n",
      "Epoch 1 Batch 7400 Loss 0.4629\n",
      "Epoch 1 Batch 7500 Loss 0.4480\n",
      "Epoch 1 Batch 7600 Loss 0.5059\n",
      "Epoch 1 Batch 7700 Loss 0.4883\n",
      "Epoch 1 Batch 7800 Loss 0.4255\n",
      "Epoch 1 Batch 7900 Loss 0.4405\n",
      "Epoch 1 Batch 8000 Loss 0.5536\n",
      "Epoch 1 Batch 8100 Loss 0.3355\n",
      "Epoch 1 Batch 8200 Loss 0.4020\n",
      "Epoch 1 Batch 8300 Loss 0.5479\n",
      "Epoch 1 Batch 8400 Loss 0.4749\n",
      "Epoch 1 Batch 8500 Loss 0.4573\n",
      "Epoch 1 Batch 8600 Loss 0.4820\n",
      "Epoch 1 Batch 8700 Loss 0.4040\n",
      "Epoch 1 Batch 8800 Loss 0.4334\n",
      "Epoch 1 Batch 8900 Loss 0.4923\n",
      "Epoch 1 Batch 9000 Loss 0.3796\n",
      "Epoch 1 Batch 9100 Loss 0.4107\n",
      "Epoch 1 Batch 9200 Loss 0.3925\n",
      "Epoch 1 Batch 9300 Loss 0.3765\n",
      "Epoch 1 Batch 9400 Loss 0.4274\n",
      "Epoch 1 Batch 9500 Loss 0.3613\n",
      "Epoch 1 Batch 9600 Loss 0.4760\n",
      "Epoch 1 Batch 9700 Loss 0.4848\n",
      "Epoch 1 Batch 9800 Loss 0.4478\n",
      "Epoch 1 Batch 9900 Loss 0.4862\n",
      "Epoch 1 Batch 10000 Loss 0.4890\n",
      "Epoch 1 Batch 10100 Loss 0.4207\n",
      "Epoch 1 Batch 10200 Loss 0.3371\n",
      "Epoch 1 Batch 10300 Loss 0.3574\n",
      "Epoch 1 Batch 10400 Loss 0.3814\n",
      "Epoch 1 Batch 10500 Loss 0.5312\n",
      "Epoch 1 Batch 10600 Loss 0.5157\n",
      "Epoch 1 Batch 10700 Loss 0.2760\n",
      "Epoch 1 Batch 10800 Loss 0.2863\n",
      "Epoch 1 Batch 10900 Loss 0.3267\n",
      "Epoch 1 Batch 11000 Loss 0.4999\n",
      "Epoch 1 Batch 11100 Loss 0.5532\n",
      "Epoch 1 Batch 11200 Loss 0.3288\n",
      "Epoch 1 Batch 11300 Loss 0.4047\n",
      "Epoch 1 Batch 11400 Loss 0.4223\n",
      "Epoch 1 Batch 11500 Loss 0.4692\n",
      "Epoch 1 Batch 11600 Loss 0.3513\n",
      "Epoch 1 Batch 11700 Loss 0.3383\n",
      "Epoch 1 Batch 11800 Loss 0.4244\n",
      "Epoch 1 Batch 11900 Loss 0.4777\n",
      "Epoch 1 Batch 12000 Loss 0.4316\n",
      "Epoch 1 Batch 12100 Loss 0.4306\n",
      "Epoch 1 Batch 12200 Loss 0.3448\n",
      "Epoch 1 Batch 12300 Loss 0.3399\n",
      "Epoch 1 Batch 12400 Loss 0.3338\n",
      "Epoch 1 Batch 12500 Loss 0.3351\n",
      "Epoch 1 Batch 12600 Loss 0.2833\n",
      "Epoch 1 Batch 12700 Loss 0.3876\n",
      "Epoch 1 Batch 12800 Loss 0.4004\n",
      "Epoch 1 Batch 12900 Loss 0.6100\n",
      "Epoch 1 Batch 13000 Loss 0.3330\n",
      "Epoch 1 Batch 13100 Loss 0.3556\n",
      "Epoch 1 Batch 13200 Loss 0.5929\n",
      "Epoch 1 Batch 13300 Loss 0.3930\n",
      "Epoch 1 Batch 13400 Loss 0.3798\n",
      "Epoch 1 Batch 13500 Loss 0.4490\n",
      "Epoch 1 Batch 13600 Loss 0.3770\n",
      "Epoch 1 Batch 13700 Loss 0.3939\n",
      "Epoch 1 Batch 13800 Loss 0.3235\n",
      "Epoch 1 Batch 13900 Loss 0.3012\n",
      "Epoch 1 Batch 14000 Loss 0.4516\n",
      "Epoch 1 Batch 14100 Loss 0.2905\n",
      "Epoch 1 Batch 14200 Loss 0.3235\n",
      "Epoch 1 Batch 14300 Loss 0.3256\n",
      "Epoch 1 Batch 14400 Loss 0.4235\n",
      "Epoch 1 Batch 14500 Loss 0.3225\n",
      "Epoch 1 Batch 14600 Loss 0.3316\n",
      "Epoch 1 Batch 14700 Loss 0.2747\n",
      "Epoch 1 Batch 14800 Loss 0.2430\n",
      "Epoch 1 Batch 14900 Loss 0.3184\n",
      "Epoch 1 Batch 15000 Loss 0.3117\n",
      "Epoch 1 Batch 15100 Loss 0.3460\n",
      "Epoch 1 Batch 15200 Loss 0.3729\n",
      "Epoch 1 Batch 15300 Loss 0.2378\n",
      "Epoch 1 Batch 15400 Loss 0.2779\n",
      "Epoch 1 Batch 15500 Loss 0.2723\n",
      "Epoch 1 Batch 15600 Loss 0.3213\n",
      "Epoch 1 Batch 15700 Loss 0.3730\n",
      "Epoch 1 Batch 15800 Loss 0.4064\n",
      "Epoch 1 Batch 15900 Loss 0.4542\n",
      "Epoch 1 Batch 16000 Loss 0.2687\n",
      "Epoch 1 Batch 16100 Loss 0.4004\n",
      "Epoch 1 Batch 16200 Loss 0.3361\n",
      "Epoch 1 Batch 16300 Loss 0.3660\n",
      "Epoch 1 Batch 16400 Loss 0.3813\n",
      "Epoch 1 Batch 16500 Loss 0.3021\n",
      "Epoch 1 Batch 16600 Loss 0.3067\n",
      "Epoch 1 Batch 16700 Loss 0.2959\n",
      "Epoch 1 Batch 16800 Loss 0.3144\n",
      "Epoch 1 Batch 16900 Loss 0.4396\n",
      "Epoch 1 Batch 17000 Loss 0.3176\n",
      "Epoch 1 Batch 17100 Loss 0.3439\n",
      "Epoch 1 Batch 17200 Loss 0.3496\n",
      "Epoch 1 Batch 17300 Loss 0.2634\n",
      "Epoch 1 Batch 17400 Loss 0.3761\n",
      "Epoch 1 Batch 17500 Loss 0.3095\n",
      "Epoch 1 Batch 17600 Loss 0.4028\n",
      "Epoch 1 Batch 17700 Loss 0.3792\n",
      "Epoch 1 Batch 17800 Loss 0.5320\n",
      "Epoch 1 Batch 17900 Loss 0.2730\n",
      "Epoch 1 Batch 18000 Loss 0.3172\n",
      "Epoch 1 Batch 18100 Loss 0.3515\n",
      "Epoch 1 Batch 18200 Loss 0.4172\n",
      "Epoch 1 Batch 18300 Loss 0.3227\n",
      "Epoch 1 Batch 18400 Loss 0.2618\n",
      "Epoch 1 Batch 18500 Loss 0.2063\n",
      "Epoch 1 Batch 18600 Loss 0.4180\n",
      "Epoch 1 Batch 18700 Loss 0.2941\n",
      "Epoch 1 Batch 18800 Loss 0.3280\n",
      "Epoch 1 Batch 18900 Loss 0.2852\n",
      "Epoch 1 Loss 0.4847\n",
      "Time taken for 1 epoch 21245.368315696716 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3649\n",
      "Epoch 2 Batch 100 Loss 0.2773\n",
      "Epoch 2 Batch 200 Loss 0.5333\n",
      "Epoch 2 Batch 300 Loss 0.2664\n",
      "Epoch 2 Batch 400 Loss 0.1949\n",
      "Epoch 2 Batch 500 Loss 0.3245\n",
      "Epoch 2 Batch 600 Loss 0.2764\n",
      "Epoch 2 Batch 700 Loss 0.3449\n",
      "Epoch 2 Batch 800 Loss 0.1970\n",
      "Epoch 2 Batch 900 Loss 0.2261\n",
      "Epoch 2 Batch 1000 Loss 0.2698\n",
      "Epoch 2 Batch 1100 Loss 0.3241\n",
      "Epoch 2 Batch 1200 Loss 0.3679\n",
      "Epoch 2 Batch 1300 Loss 0.3481\n",
      "Epoch 2 Batch 1400 Loss 0.3099\n",
      "Epoch 2 Batch 1500 Loss 0.3266\n",
      "Epoch 2 Batch 1600 Loss 0.3554\n",
      "Epoch 2 Batch 1700 Loss 0.2079\n",
      "Epoch 2 Batch 1800 Loss 0.3956\n",
      "Epoch 2 Batch 1900 Loss 0.4408\n",
      "Epoch 2 Batch 2000 Loss 0.2900\n",
      "Epoch 2 Batch 2100 Loss 0.3165\n",
      "Epoch 2 Batch 2200 Loss 0.2592\n",
      "Epoch 2 Batch 2300 Loss 0.3033\n",
      "Epoch 2 Batch 2400 Loss 0.4669\n",
      "Epoch 2 Batch 2500 Loss 0.1898\n",
      "Epoch 2 Batch 2600 Loss 0.2343\n",
      "Epoch 2 Batch 2700 Loss 0.2666\n",
      "Epoch 2 Batch 2800 Loss 0.3114\n",
      "Epoch 2 Batch 2900 Loss 0.4844\n",
      "Epoch 2 Batch 3000 Loss 0.4422\n",
      "Epoch 2 Batch 3100 Loss 0.2250\n",
      "Epoch 2 Batch 3200 Loss 0.2781\n",
      "Epoch 2 Batch 3300 Loss 0.1242\n",
      "Epoch 2 Batch 3400 Loss 0.2956\n",
      "Epoch 2 Batch 3500 Loss 0.2511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 3600 Loss 0.2606\n",
      "Epoch 2 Batch 3700 Loss 0.1620\n",
      "Epoch 2 Batch 3800 Loss 0.2699\n",
      "Epoch 2 Batch 3900 Loss 0.3320\n",
      "Epoch 2 Batch 4000 Loss 0.1488\n",
      "Epoch 2 Batch 4100 Loss 0.2304\n",
      "Epoch 2 Batch 4200 Loss 0.2511\n",
      "Epoch 2 Batch 4300 Loss 0.2565\n",
      "Epoch 2 Batch 4400 Loss 0.1473\n",
      "Epoch 2 Batch 4500 Loss 0.1774\n",
      "Epoch 2 Batch 4600 Loss 0.3787\n",
      "Epoch 2 Batch 4700 Loss 0.1904\n",
      "Epoch 2 Batch 4800 Loss 0.2569\n",
      "Epoch 2 Batch 4900 Loss 0.2456\n",
      "Epoch 2 Batch 5000 Loss 0.2134\n",
      "Epoch 2 Batch 5100 Loss 0.3293\n",
      "Epoch 2 Batch 5200 Loss 0.2635\n",
      "Epoch 2 Batch 5300 Loss 0.2385\n",
      "Epoch 2 Batch 5400 Loss 0.2486\n",
      "Epoch 2 Batch 5500 Loss 0.2127\n",
      "Epoch 2 Batch 5600 Loss 0.2543\n",
      "Epoch 2 Batch 5700 Loss 0.2427\n",
      "Epoch 2 Batch 5800 Loss 0.3561\n",
      "Epoch 2 Batch 5900 Loss 0.2391\n",
      "Epoch 2 Batch 6000 Loss 0.2143\n",
      "Epoch 2 Batch 6100 Loss 0.2420\n",
      "Epoch 2 Batch 6200 Loss 0.3197\n",
      "Epoch 2 Batch 6300 Loss 0.3472\n",
      "Epoch 2 Batch 6400 Loss 0.2320\n",
      "Epoch 2 Batch 6500 Loss 0.1716\n",
      "Epoch 2 Batch 6600 Loss 0.2341\n",
      "Epoch 2 Batch 6700 Loss 0.3643\n",
      "Epoch 2 Batch 6800 Loss 0.3693\n",
      "Epoch 2 Batch 6900 Loss 0.2295\n",
      "Epoch 2 Batch 7000 Loss 0.1967\n",
      "Epoch 2 Batch 7100 Loss 0.2412\n",
      "Epoch 2 Batch 7200 Loss 0.2952\n",
      "Epoch 2 Batch 7300 Loss 0.2187\n",
      "Epoch 2 Batch 7400 Loss 0.1975\n",
      "Epoch 2 Batch 7500 Loss 0.2107\n",
      "Epoch 2 Batch 7600 Loss 0.2529\n",
      "Epoch 2 Batch 7700 Loss 0.2785\n",
      "Epoch 2 Batch 7800 Loss 0.1819\n",
      "Epoch 2 Batch 7900 Loss 0.1865\n",
      "Epoch 2 Batch 8000 Loss 0.2617\n",
      "Epoch 2 Batch 8100 Loss 0.1563\n",
      "Epoch 2 Batch 8200 Loss 0.1881\n",
      "Epoch 2 Batch 8300 Loss 0.3043\n",
      "Epoch 2 Batch 8400 Loss 0.2021\n",
      "Epoch 2 Batch 8500 Loss 0.1989\n",
      "Epoch 2 Batch 8600 Loss 0.1982\n",
      "Epoch 2 Batch 8700 Loss 0.2240\n",
      "Epoch 2 Batch 8800 Loss 0.2258\n",
      "Epoch 2 Batch 8900 Loss 0.2699\n",
      "Epoch 2 Batch 9000 Loss 0.1782\n",
      "Epoch 2 Batch 9100 Loss 0.2044\n",
      "Epoch 2 Batch 9200 Loss 0.2021\n",
      "Epoch 2 Batch 9300 Loss 0.2165\n",
      "Epoch 2 Batch 9400 Loss 0.2216\n",
      "Epoch 2 Batch 9500 Loss 0.2198\n",
      "Epoch 2 Batch 9600 Loss 0.2172\n",
      "Epoch 2 Batch 9700 Loss 0.2768\n",
      "Epoch 2 Batch 9800 Loss 0.2790\n",
      "Epoch 2 Batch 9900 Loss 0.2644\n",
      "Epoch 2 Batch 10000 Loss 0.2585\n",
      "Epoch 2 Batch 10100 Loss 0.2113\n",
      "Epoch 2 Batch 10200 Loss 0.1633\n",
      "Epoch 2 Batch 10300 Loss 0.1771\n",
      "Epoch 2 Batch 10400 Loss 0.2089\n",
      "Epoch 2 Batch 10500 Loss 0.3230\n",
      "Epoch 2 Batch 10600 Loss 0.3105\n",
      "Epoch 2 Batch 10700 Loss 0.1289\n",
      "Epoch 2 Batch 10800 Loss 0.1449\n",
      "Epoch 2 Batch 10900 Loss 0.1674\n",
      "Epoch 2 Batch 11000 Loss 0.2597\n",
      "Epoch 2 Batch 11100 Loss 0.3233\n",
      "Epoch 2 Batch 11200 Loss 0.1760\n",
      "Epoch 2 Batch 11300 Loss 0.2592\n",
      "Epoch 2 Batch 11400 Loss 0.2473\n",
      "Epoch 2 Batch 11500 Loss 0.2891\n",
      "Epoch 2 Batch 11600 Loss 0.2241\n",
      "Epoch 2 Batch 11700 Loss 0.1379\n",
      "Epoch 2 Batch 11800 Loss 0.2497\n",
      "Epoch 2 Batch 11900 Loss 0.3007\n",
      "Epoch 2 Batch 12000 Loss 0.2423\n",
      "Epoch 2 Batch 12100 Loss 0.2271\n",
      "Epoch 2 Batch 12200 Loss 0.2086\n",
      "Epoch 2 Batch 12300 Loss 0.1784\n",
      "Epoch 2 Batch 12400 Loss 0.1941\n",
      "Epoch 2 Batch 12500 Loss 0.1938\n",
      "Epoch 2 Batch 12600 Loss 0.1653\n",
      "Epoch 2 Batch 12700 Loss 0.2570\n",
      "Epoch 2 Batch 12800 Loss 0.2531\n",
      "Epoch 2 Batch 12900 Loss 0.3911\n",
      "Epoch 2 Batch 13000 Loss 0.1954\n",
      "Epoch 2 Batch 13100 Loss 0.2069\n",
      "Epoch 2 Batch 13200 Loss 0.4173\n",
      "Epoch 2 Batch 13300 Loss 0.2092\n",
      "Epoch 2 Batch 13400 Loss 0.2368\n",
      "Epoch 2 Batch 13500 Loss 0.3038\n",
      "Epoch 2 Batch 13600 Loss 0.2668\n",
      "Epoch 2 Batch 13700 Loss 0.2257\n",
      "Epoch 2 Batch 13800 Loss 0.1967\n",
      "Epoch 2 Batch 13900 Loss 0.1498\n",
      "Epoch 2 Batch 14000 Loss 0.3050\n",
      "Epoch 2 Batch 14100 Loss 0.1797\n",
      "Epoch 2 Batch 14200 Loss 0.1895\n",
      "Epoch 2 Batch 14300 Loss 0.1936\n",
      "Epoch 2 Batch 14400 Loss 0.2490\n",
      "Epoch 2 Batch 14500 Loss 0.2236\n",
      "Epoch 2 Batch 14600 Loss 0.2384\n",
      "Epoch 2 Batch 14700 Loss 0.1614\n",
      "Epoch 2 Batch 14800 Loss 0.1161\n",
      "Epoch 2 Batch 14900 Loss 0.1821\n",
      "Epoch 2 Batch 15000 Loss 0.2036\n",
      "Epoch 2 Batch 15100 Loss 0.1839\n",
      "Epoch 2 Batch 15200 Loss 0.2151\n",
      "Epoch 2 Batch 15300 Loss 0.1295\n",
      "Epoch 2 Batch 15400 Loss 0.1625\n",
      "Epoch 2 Batch 15500 Loss 0.1887\n",
      "Epoch 2 Batch 15600 Loss 0.2083\n",
      "Epoch 2 Batch 15700 Loss 0.2033\n",
      "Epoch 2 Batch 15800 Loss 0.2714\n",
      "Epoch 2 Batch 15900 Loss 0.3264\n",
      "Epoch 2 Batch 16000 Loss 0.1591\n",
      "Epoch 2 Batch 16100 Loss 0.2333\n",
      "Epoch 2 Batch 16200 Loss 0.2220\n",
      "Epoch 2 Batch 16300 Loss 0.2246\n",
      "Epoch 2 Batch 16400 Loss 0.2259\n",
      "Epoch 2 Batch 16500 Loss 0.1912\n",
      "Epoch 2 Batch 16600 Loss 0.1853\n",
      "Epoch 2 Batch 16700 Loss 0.1813\n",
      "Epoch 2 Batch 16800 Loss 0.2163\n",
      "Epoch 2 Batch 16900 Loss 0.3198\n",
      "Epoch 2 Batch 17000 Loss 0.2371\n",
      "Epoch 2 Batch 17100 Loss 0.2115\n",
      "Epoch 2 Batch 17200 Loss 0.2228\n",
      "Epoch 2 Batch 17300 Loss 0.1592\n",
      "Epoch 2 Batch 17400 Loss 0.2473\n",
      "Epoch 2 Batch 17500 Loss 0.2206\n",
      "Epoch 2 Batch 17600 Loss 0.2207\n",
      "Epoch 2 Batch 17700 Loss 0.2616\n",
      "Epoch 2 Batch 17800 Loss 0.4150\n",
      "Epoch 2 Batch 17900 Loss 0.1813\n",
      "Epoch 2 Batch 18000 Loss 0.2090\n",
      "Epoch 2 Batch 18100 Loss 0.2355\n",
      "Epoch 2 Batch 18200 Loss 0.2516\n",
      "Epoch 2 Batch 18300 Loss 0.2272\n",
      "Epoch 2 Batch 18400 Loss 0.1705\n",
      "Epoch 2 Batch 18500 Loss 0.1130\n",
      "Epoch 2 Batch 18600 Loss 0.3085\n",
      "Epoch 2 Batch 18700 Loss 0.2251\n",
      "Epoch 2 Batch 18800 Loss 0.2083\n",
      "Epoch 2 Batch 18900 Loss 0.1710\n",
      "Epoch 2 Loss 0.2476\n",
      "Time taken for 1 epoch 21232.436054229736 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2282\n",
      "Epoch 3 Batch 100 Loss 0.1948\n",
      "Epoch 3 Batch 200 Loss 0.3780\n",
      "Epoch 3 Batch 300 Loss 0.1681\n",
      "Epoch 3 Batch 400 Loss 0.1330\n",
      "Epoch 3 Batch 500 Loss 0.2279\n",
      "Epoch 3 Batch 600 Loss 0.2027\n",
      "Epoch 3 Batch 700 Loss 0.2256\n",
      "Epoch 3 Batch 800 Loss 0.1320\n",
      "Epoch 3 Batch 900 Loss 0.1395\n",
      "Epoch 3 Batch 1000 Loss 0.1740\n",
      "Epoch 3 Batch 1100 Loss 0.2206\n",
      "Epoch 3 Batch 1200 Loss 0.2636\n",
      "Epoch 3 Batch 1300 Loss 0.2362\n",
      "Epoch 3 Batch 1400 Loss 0.2079\n",
      "Epoch 3 Batch 1500 Loss 0.2056\n",
      "Epoch 3 Batch 1600 Loss 0.2553\n",
      "Epoch 3 Batch 1700 Loss 0.1481\n",
      "Epoch 3 Batch 1800 Loss 0.2765\n",
      "Epoch 3 Batch 1900 Loss 0.2827\n",
      "Epoch 3 Batch 2000 Loss 0.1906\n",
      "Epoch 3 Batch 2100 Loss 0.2150\n",
      "Epoch 3 Batch 2200 Loss 0.1899\n",
      "Epoch 3 Batch 2300 Loss 0.1960\n",
      "Epoch 3 Batch 2400 Loss 0.3002\n",
      "Epoch 3 Batch 2500 Loss 0.1386\n",
      "Epoch 3 Batch 2600 Loss 0.1384\n",
      "Epoch 3 Batch 2700 Loss 0.2188\n",
      "Epoch 3 Batch 2800 Loss 0.1924\n",
      "Epoch 3 Batch 2900 Loss 0.3704\n",
      "Epoch 3 Batch 3000 Loss 0.3102\n",
      "Epoch 3 Batch 3100 Loss 0.1406\n",
      "Epoch 3 Batch 3200 Loss 0.2013\n",
      "Epoch 3 Batch 3300 Loss 0.0977\n",
      "Epoch 3 Batch 3400 Loss 0.1816\n",
      "Epoch 3 Batch 3500 Loss 0.1803\n",
      "Epoch 3 Batch 3600 Loss 0.2011\n",
      "Epoch 3 Batch 3700 Loss 0.0937\n",
      "Epoch 3 Batch 3800 Loss 0.1664\n",
      "Epoch 3 Batch 3900 Loss 0.2513\n",
      "Epoch 3 Batch 4000 Loss 0.0983\n",
      "Epoch 3 Batch 4100 Loss 0.1404\n",
      "Epoch 3 Batch 4200 Loss 0.1577\n",
      "Epoch 3 Batch 4300 Loss 0.1839\n",
      "Epoch 3 Batch 4400 Loss 0.1050\n",
      "Epoch 3 Batch 4500 Loss 0.1195\n",
      "Epoch 3 Batch 4600 Loss 0.2486\n",
      "Epoch 3 Batch 4700 Loss 0.1471\n",
      "Epoch 3 Batch 4800 Loss 0.1578\n",
      "Epoch 3 Batch 4900 Loss 0.1799\n",
      "Epoch 3 Batch 5000 Loss 0.1613\n",
      "Epoch 3 Batch 5100 Loss 0.2332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3dd933de298c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rus-eng/eng_anki_train.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rus-eng/ru_anki_train.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\coding\\deeplavrov\\deeplavrov\\models\\wordlevel.py\u001b[0m in \u001b[0;36mfit_from_file\u001b[1;34m(self, input_file, target_file, val_input_file, val_target_file)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m                 \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\coding\\deeplavrov\\deeplavrov\\models\\wordlevel.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, inp, targ, enc_hidden)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;31m# teacher forcing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;31m# Eager execution on data tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\coding\\deeplavrov\\deeplavrov\\models\\wordlevel.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;31m# Eager execution on data tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\cudnn_recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    109\u001b[0m       \u001b[1;31m# Reverse time axis.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\cudnn_recurrent.py\u001b[0m in \u001b[0;36m_process_batch\u001b[1;34m(self, inputs, initial_state)\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         ],\n\u001b[1;32m--> 295\u001b[1;33m         shape=self._vector_shape)\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     outputs, h, _, _ = gen_cudnn_rnn_ops.cudnn_rnn(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_canonical_to_params\u001b[1;34m(weights, biases, shape, transpose_weights)\u001b[0m\n\u001b[0;32m   2803\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtranspose_weights\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2805\u001b[1;33m   \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2806\u001b[0m   \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2807\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2803\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtranspose_weights\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2805\u001b[1;33m   \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2806\u001b[0m   \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2807\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   7154\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   7155\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Reshape\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7156\u001b[1;33m         name, _ctx._post_execution_callbacks, tensor, shape)\n\u001b[0m\u001b[0;32m   7157\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7158\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "translator.fit_from_file(input_file='rus-eng/eng_anki_train.txt', target_file='rus-eng/ru_anki_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "translator.save('model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет', '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate('hello.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['у', 'него', 'есть', 'фотоаппарат', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate('He has a camera.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['я', 'смотрела', 'телевизор', ',', 'когда', 'зазвонил', 'телефон', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate('I was watching television when the telephone rang.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ты', 'уже', 'слышал', 'эту', 'шутку', '?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate(\"Have you heard this joke before?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = AttentionRNNTranslator.load('model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.num_epochs, t2.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет', '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.translate('hello.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['у', 'него', 'есть', 'фотоаппарат', '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.translate('He has a camera.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['я', 'смотрела', 'телевизор', ',', 'когда', 'зазвонил', 'телефон', '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.translate('I was watching television when the telephone rang.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ты', 'уже', 'слышал', 'эту', 'шутку', '?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.translate(\"Have you heard this joke before?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['том',\n",
       " 'не',\n",
       " 'хочет',\n",
       " 'провести',\n",
       " 'остаток',\n",
       " 'своей',\n",
       " 'жизни',\n",
       " 'в',\n",
       " 'тюрьме',\n",
       " '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.translate(\"Tom doesn't want to spend the rest of his life in prison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
